services:
  db:
    image: postgres:latest
    env_file:
      - .env
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-postgre}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgre}
      - POSTGRES_DB=${POSTGRES_DB:-postgre}
    ports:
      - "5432:5432"
    volumes:
      - ./volumes/postgres/data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgre}" ]
      interval: 5s
      timeout: 5s
      retries: 5

  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server
    restart: unless-stopped
    ports:
      - "8080:8080"
    env_file:
      - .env
    environment:
      - HF_MODEL=${HF_MODEL:-ggml-org/SmolVLM-500M-Instruct-GGUF}
    command: >
      -hf ${HF_MODEL} --host 0.0.0.0 --port 8080 --ctx-size 2048 --batch-size 512 --verbose --mlock
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s

  api:
    build:
      context: .
      dockerfile: Dockerfile
    restart: unless-stopped
    env_file:
      - .env
    environment:
      - NODE_ENV=${NODE_ENV:-production}
      - PORT=${PORT:-3000}
      - DATABASE_URL=postgresql://${POSTGRES_USER:-postgre}:${POSTGRES_PASSWORD:-postgre}@db:5432/${POSTGRES_DB:-postgre}?schema=public
      - LLAMA_SERVER_URL=http://llama-server:8080
    ports:
      - "${PORT:-3000}:3000"
    depends_on:
      db:
        condition: service_healthy
      llama-server:
        condition: service_healthy
    volumes:
      - ./client-demo:/usr/src/app/client-demo
      - ./uploads:/usr/src/app/uploads
    command: >
      sh -c "npx prisma migrate deploy && npm start"

volumes:
  postgres_data:
